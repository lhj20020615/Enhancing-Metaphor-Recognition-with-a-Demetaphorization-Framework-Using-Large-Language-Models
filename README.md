# Enhancing-Metaphor-Recognition-with-a-Demetaphorization-Framework-Using-Large-Language-Models
“This is a code repository for the paper, containing the essential implementation components. Redundant parts have been removed, and the accompanying documentation provides corresponding solutions and replacement strategies. We recommend that you conduct the experiment in accordance with these resources.”
“The first dataset we provide is a cleaned corpus specifically designed for the demetaphorization experiment, and it is used in this study as the demetaphorization dataset. This dataset is composed of three parts: (i) labels (all set to 1, denoted as 1.tsv), (ii) the positions of the corresponding nominal metaphors within the sentences, and (iii) the original sentences.”
A masked version of this dataset has been created, where the ‘tentor’ column represents the tenor, derived from the ‘Definition’ field in the original dataset. The first dataset is processed using a WordNet-based large language model in a zero-shot setting to clean nominal metaphors in VUA-18. The target words are confirmed nominal metaphors and form the experimental set for nominal metaphor analysis.

The initial training set for experimental validation is constructed from all metaphorical data in 1.tsv, supplemented with an equal amount of non-metaphorical data from 0.tsv. Validation and test sets are created using equal proportions of carefully selected metaphorical and non-metaphorical data. Since the demetaphorization accuracy reaches at most around 70%, we filtered out all non-metaphorical sentences after demetaphorization, resulting in a final collection of over 1,300 confirmed non-metaphorical instances.

The complete training set incorporates matched demetaphorized instances proportionally into the original training data. The final dataset and the training code are provided, with models uploaded and downloadable via the Download.py script. A script is also provided to select training instances with the highest accuracy (ACC) to reproduce the results presented in the results folder.

All metaphor annotations, including zero-shot annotations, were generated using this code. Cross-validation was performed using DeepSeek V3 and GPT-4o. Masking was applied to the provided metaphorical sentences. The zero-shot verification component can be used iteratively until a satisfactory performance level is achieved; in this study, it was used once.

For non-metaphorical replacement, based on the tenor and sentence context, large language models were used to generate specific words to replace the masked portions. Five models were employed; any model can be substituted, and multiple runs can generate several candidate words for insertion and annotation.

Finally, a nominal metaphor identification component is provided to determine whether each instance is a nominal metaphor. Multiple models should be used, and additional verification is required for cases where model outputs differ.
